{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "Rlx1NTkObp2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkrvltJkNeUy",
        "outputId": "22a5c2b8-8034-4a54-f7ad-91d311895269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6xmQngzHpuf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(f'/content/drive/MyDrive/final.pkl', 'rb') as f:\n",
        "  target_texts = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIYyvjTk4REw",
        "outputId": "d9939eab-1cf5-4941-fa8e-73deefc0d403"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfadUbMIIM0n",
        "outputId": "c22c760c-ac89-4575-cb61-55674e41f022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Da Nang\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "REMOVE_DIACRITIC_TABLE = str.maketrans(\n",
        "    \"ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬĐÈÉẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴáàảãạăắằẳẵặâấầẩẫậđèéẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵ\",\n",
        "    \"A\" * 17 + \"D\" + \"E\" * 11 + \"I\" * 5 + \"O\" * 17 + \"U\" * 11 + \"Y\" * 5 + \"a\" * 17 + \"d\" + \"e\" * 11 + \"i\" * 5 + \"o\" * 17 + \"u\" * 11 + \"y\" * 5\n",
        ")\n",
        "\n",
        "\n",
        "def remove_diacritic(txt: str) -> str:\n",
        "  if not unicodedata.is_normalized(\"NFC\", txt):\n",
        "    txt = unicodedata.normalize(\"NFC\", txt)\n",
        "  return txt.translate(REMOVE_DIACRITIC_TABLE)\n",
        "\n",
        "print(remove_diacritic(\"Đà Nẵng\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1bsEOGS4acZ"
      },
      "outputs": [],
      "source": [
        "input_texts = [remove_diacritic(sentence) for sentence in target_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjDgU14g4lzX",
        "outputId": "6f4127f2-385c-498e-81da-54cdd9d46586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Giam can chi don thuan la giam di trong luong co the co the tu viec an kieng nhin an hoac tinh trang benh ly nao do',\n",
              " 'Co nguoi con kem theo cac trieu chung toan than nhu met moi buon ngu phu mat tu tuong tam trang khong on dinh',\n",
              " 'Cach mang mau voi nhieu ten goi khac nhau nhu',\n",
              " 'Ong Tien co mot nguoi ban ten la Duoc co can nha lau sat ben canh',\n",
              " 'Toi co cam tuong nhu phim duoc lam rat thoang co cam hung tu nhien nhu mot luong gio moi chua day oxy ma khan gia co the hit day long nguc',\n",
              " 'Can cu Hien phap nuoc Cong hoa xa hoi chu nghia Viet Nam;']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "input_texts[1:-1:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H1sYH6kKX5d"
      },
      "outputs": [],
      "source": [
        "# Lower the text from input and output\n",
        "input_texts = [txt.lower() for txt in input_texts]\n",
        "target_texts = [txt.lower() for txt in target_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzwBBw0RKdDG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NS6VVhFTp1F"
      },
      "outputs": [],
      "source": [
        "# Tokenize the inputs\n",
        "MAX_NUM_WORDS = 30000\n",
        "tokenizer_input = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_input.fit_on_texts(input_texts)\n",
        "input_sequences = tokenizer_input.texts_to_sequences(input_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-njUl2lKTxDb",
        "outputId": "d8d88eb5-b1bf-4f73-b54a-563bc3d6fac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22219 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "# Get the word to index mapping for input\n",
        "word2idx_input = tokenizer_input.word_index\n",
        "print('Found %s unique tokens.' % len(word2idx_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nenDxKM7cNFi",
        "outputId": "7a97b438-7a0b-4365-ab5a-2b5588dc6a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max input length: 411\n"
          ]
        }
      ],
      "source": [
        "# determine max length input sequence\n",
        "max_len_input = max(len(s) for s in input_sequences)\n",
        "print('Max input length:', max_len_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MGz0OPhdLGX",
        "outputId": "1f433291-20b7-4f8f-f5aa-5a3c1bf554f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input tensor: (300000, 411)\n"
          ]
        }
      ],
      "source": [
        "# pad the sequences to NxT matrix\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len_input, padding='post', value=0)\n",
        "print('Shape of input tensor:', input_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG0AiorBffXI"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from targets to target labels\n",
        "import re\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    return re.sub(r'[^\\w\\s]', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRqvh15eiNzG"
      },
      "outputs": [],
      "source": [
        "target_texts = [remove_punctuation(txt) for txt in target_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhaDzJjTiSJ2"
      },
      "outputs": [],
      "source": [
        "word2idx = {}\n",
        "for text in target_texts:\n",
        "  data = text.split(' ')\n",
        "  for word in data:\n",
        "    remove_diacritic_word = remove_diacritic(word)\n",
        "    if remove_diacritic_word not in word2idx:\n",
        "      word2idx[remove_diacritic_word] = {word: 1}\n",
        "    else:\n",
        "      if word not in word2idx[remove_diacritic_word]:\n",
        "        word2idx[remove_diacritic_word][word] = len(word2idx[remove_diacritic_word]) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUsWcmikibbU"
      },
      "outputs": [],
      "source": [
        "# Tokenizet the output\n",
        "MAX_NUM_WORDS = 60000\n",
        "tokenizer_target = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer_target.fit_on_texts(target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvLymLWjlsUc",
        "outputId": "45f1774a-ddfc-432c-ad04-2d95136a8579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25592 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "# Get the word to index mapping for target\n",
        "word2idx_target = tokenizer_target.word_index\n",
        "print('Found %s unique tokens.' % len(word2idx_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxFAdTAPltBx"
      },
      "outputs": [],
      "source": [
        "target_labels = []\n",
        "for text in target_texts:\n",
        "  label = []\n",
        "  data = text.split(' ')\n",
        "  for word in data:\n",
        "    remove_diacritic_word = remove_diacritic(word)\n",
        "    label.append(word2idx[remove_diacritic_word][word])\n",
        "  target_labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucIBLHqXl6-e",
        "outputId": "6d903c0d-2fbd-4b57-9ce7-3011748c3920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of target tensor: (300000, 411)\n"
          ]
        }
      ],
      "source": [
        "target_labels = pad_sequences(target_labels, maxlen=max_len_input, padding='post', value=0)\n",
        "print('Shape of target tensor:', target_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho1EzkmMl_se"
      },
      "outputs": [],
      "source": [
        "num_labels = max(max(label) for label in target_labels) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW4Uy-P2mEGc",
        "outputId": "0f90e8de-764e-4b33-dde4-da7240fc9472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "num_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHhPblkLmHNO"
      },
      "source": [
        "Create an index-to-word Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9U93vjcmK2B"
      },
      "outputs": [],
      "source": [
        "idx2word = {}\n",
        "for key in word2idx.keys():\n",
        "  idx2word[key] = {}\n",
        "  for word in word2idx[key].keys():\n",
        "    idx2word[key][word2idx[key][word]] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTrM3wy9ECX"
      },
      "source": [
        "Save the input and output tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_E1fZSSL9I_x"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(f'tokenizer_input_{style}.pkl', 'wb') as handle:\n",
        "  pickle.dump(tokenizer_input, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open(f'tokenizer_target_{style}.pkl', 'wb') as handle:\n",
        "  pickle.dump(tokenizer_target, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCIwppzwruA5"
      },
      "outputs": [],
      "source": [
        "with open(f'idx2word_{style}.pkl', 'wb') as handle:\n",
        "  pickle.dump(idx2word, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzWHebgXmcxm"
      },
      "source": [
        "Deploy the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "669FRL-hmeoW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
        "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vGEzlwhmlM4"
      },
      "outputs": [],
      "source": [
        "num_words = min(MAX_NUM_WORDS, len(word2idx_input) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU4V6qeTfkFK",
        "outputId": "bafed654-bfc6-4c5c-fe60-b4192ae8bf65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22220"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "num_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiRhF5r0mm7W"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 200\n",
        "# create embedding layer\n",
        "embedding_layer = Embedding(\n",
        "    num_words,\n",
        "    EMBEDDING_DIM,\n",
        "    input_length=max_len_input,\n",
        "    trainable=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihcoqtuSm7P8",
        "outputId": "05d7c1be-4339-4cbe-8ddd-e263eae3863b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 411)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 411, 200)          4444000   \n",
            "                                                                 \n",
            " spatial_dropout1d (Spatial  (None, 411, 200)          0         \n",
            " Dropout1D)                                                      \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 411, 100)          100400    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 411, 30)           3030      \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4547430 (17.35 MB)\n",
            "Trainable params: 4547430 (17.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=(max_len_input,))\n",
        "x = embedding_layer(input)\n",
        "model = SpatialDropout1D(0.1)(x)\n",
        "model = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_dropout=0.1))(model)\n",
        "out = TimeDistributed(Dense(num_labels, activation=\"softmax\"))(model)\n",
        "model = Model(input, out)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVG9lKj7ndnl"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFAY97q0nfop"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjI2VYR2ng9n"
      },
      "outputs": [],
      "source": [
        "# Model checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    f'BiLSTM_model.h5', verbose=1, save_weights_only=True)\n",
        "# Callback for early stopping\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFQPahQso0Dx"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "VALIDATION_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEoYml32pDor",
        "outputId": "55c37d95-a369-4be5-f496-589848ed572a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.9299\n",
            "Epoch 1: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 203s 628ms/step - loss: 0.3596 - accuracy: 0.9299 - val_loss: 0.1949 - val_accuracy: 0.9428\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9549\n",
            "Epoch 2: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 192s 613ms/step - loss: 0.1411 - accuracy: 0.9549 - val_loss: 0.1687 - val_accuracy: 0.9480\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9587\n",
            "Epoch 3: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 204s 652ms/step - loss: 0.1259 - accuracy: 0.9587 - val_loss: 0.1629 - val_accuracy: 0.9497\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9594\n",
            "Epoch 4: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 193s 618ms/step - loss: 0.1215 - accuracy: 0.9594 - val_loss: 0.1610 - val_accuracy: 0.9502\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9599\n",
            "Epoch 5: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 195s 623ms/step - loss: 0.1184 - accuracy: 0.9599 - val_loss: 0.1597 - val_accuracy: 0.9504\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9603\n",
            "Epoch 6: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 199s 636ms/step - loss: 0.1161 - accuracy: 0.9603 - val_loss: 0.1584 - val_accuracy: 0.9508\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9608\n",
            "Epoch 7: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 208s 666ms/step - loss: 0.1140 - accuracy: 0.9608 - val_loss: 0.1582 - val_accuracy: 0.9507\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9614\n",
            "Epoch 8: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 202s 644ms/step - loss: 0.1120 - accuracy: 0.9614 - val_loss: 0.1572 - val_accuracy: 0.9509\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9619\n",
            "Epoch 9: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 201s 643ms/step - loss: 0.1102 - accuracy: 0.9619 - val_loss: 0.1577 - val_accuracy: 0.9511\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9625\n",
            "Epoch 10: saving model to BiLSTM_model_artistic.h5\n",
            "313/313 [==============================] - 211s 673ms/step - loss: 0.1085 - accuracy: 0.9625 - val_loss: 0.1572 - val_accuracy: 0.9515\n"
          ]
        }
      ],
      "source": [
        "print('Training model...')\n",
        "r = model.fit(\n",
        "    input_sequences,\n",
        "    target_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    callbacks = [cp_callback, es_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak07NvS5atM_"
      },
      "source": [
        "Prediction Phase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.history['loss']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ2dg-SlagYt",
        "outputId": "114864f4-4044-4faf-fd8b-d4fbe55709b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35957053303718567,\n",
              " 0.14108815789222717,\n",
              " 0.12585732340812683,\n",
              " 0.12151019275188446,\n",
              " 0.11843199282884598,\n",
              " 0.1161467656493187,\n",
              " 0.11397743970155716,\n",
              " 0.11195237934589386,\n",
              " 0.11020161211490631,\n",
              " 0.10853244364261627]"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.history['val_loss']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xydxxBzLa3SX",
        "outputId": "9235eccb-e5aa-4f16-d7b4-434e4e3893fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19487278163433075,\n",
              " 0.16865253448486328,\n",
              " 0.16291475296020508,\n",
              " 0.16097094118595123,\n",
              " 0.15967343747615814,\n",
              " 0.1584356278181076,\n",
              " 0.1581593006849289,\n",
              " 0.15716703236103058,\n",
              " 0.15765069425106049,\n",
              " 0.1572256088256836]"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r.history['val_accuracy']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNljMAwZa6fx",
        "outputId": "29950aa3-7b22-499b-f517-ecb606339379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9427601099014282,\n",
              " 0.9480058550834656,\n",
              " 0.9496764540672302,\n",
              " 0.950194776058197,\n",
              " 0.9503509998321533,\n",
              " 0.9508346319198608,\n",
              " 0.9507274627685547,\n",
              " 0.9508797526359558,\n",
              " 0.9510581493377686,\n",
              " 0.9515072107315063]"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'results.pkl', 'wb') as file:\n",
        "  pickle.dump([r.history['loss'], r.history['val_loss'], r.history['val_accuracy']], file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "xaw1E3uecEiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO5AoB1XawkF"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgu1YYHUVWZp"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_sequences(text):\n",
        "  text_sequences = tokenizer_input.texts_to_sequences(text)\n",
        "  print(text_sequences)\n",
        "  text_sequences = pad_sequences(text_sequences, maxlen=max_len_input, padding='post', value=0)\n",
        "  return text_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xty2NbZUFE9"
      },
      "outputs": [],
      "source": [
        "def predict(input_texts:list):\n",
        "    sequence = convert_text_to_sequences(input_texts)\n",
        "    p = np.argmax(model.predict(sequence), axis=-1)\n",
        "    output_texts = []\n",
        "    for i in range(len(input_texts)):\n",
        "        predict_output = \"\"\n",
        "        user_query_split = remove_punctuation(input_texts[i])\n",
        "        user_query_split = user_query_split.lower().split(\" \")\n",
        "        for t in range(len(user_query_split)):\n",
        "            try:\n",
        "                predict_output += str(idx2word[user_query_split[t]][p[i][t]]) + \" \"\n",
        "            except KeyError:\n",
        "                predict_output += user_query_split[t] + \" \"\n",
        "        output_texts.append(predict_output[:-1])\n",
        "    return output_texts"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}